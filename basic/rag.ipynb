{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2:latest\")\n",
    "\n",
    "Settings.llm = llm\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron 2 documento(s).\n"
     ]
    }
   ],
   "source": [
    "# Cargar documentos\n",
    "data_dir = \"./data\"\n",
    "try:\n",
    "    documents = SimpleDirectoryReader(data_dir).load_data()\n",
    "    if not documents:\n",
    "        print(f\"No se encontraron documentos en {data_dir}\")\n",
    "        exit()\n",
    "    print(f\"Se cargaron {len(documents)} documento(s).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando documentos: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando el índice...\n",
      "Índice creado.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creando el índice...\")\n",
    "try:\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    print(\"Índice creado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creando el índice: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever object at 0x147c6c210>\n"
     ]
    }
   ],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=1,  # Recuperar los 3 fragmentos más similares\n",
    ")\n",
    "\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fase de Recuperación para: '¿Qué es Langchain?' ---\n",
      "Se recuperaron 1 nodos:\n",
      "\n",
      "Nodo 1 (Score: 0.6868):\n",
      "Langchain es otro framework popular para construir aplicaciones con LLMs.\n",
      "Ofrece componentes modulares como cadenas, agentes y herramientas de memoria.\n",
      "Se puede integrar con Llama Index para flujos de trabajo RAG más complejos.\n",
      "Crew AI y LangGraph so...\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "consulta_test = \"¿Qué es Langchain?\"\n",
    "nodos_recuperados = retriever.retrieve(consulta_test)\n",
    "\n",
    "print(f\"\\n--- Fase de Recuperación para: '{consulta_test}' ---\")\n",
    "if nodos_recuperados:\n",
    "    print(f\"Se recuperaron {len(nodos_recuperados)} nodos:\")\n",
    "    for i, nodo in enumerate(nodos_recuperados):\n",
    "        print(f\"\\nNodo {i+1} (Score: {nodo.score:.4f}):\")\n",
    "        print(nodo.get_content()[:250] + \"...\") # Imprime los primeros 250 caracteres del nodo\n",
    "else:\n",
    "    print(\"No se recuperaron nodos.\")\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"refine\" # Puedes experimentar con diferentes modos\n",
    ")\n",
    "\n",
    "query_engine_rag = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fase de Generación Aumentada para: 'Explícame brevemente qué es Langchain y cómo se relaciona con Llama Index.' ---\n"
     ]
    }
   ],
   "source": [
    "pregunta_rag = \"Explícame brevemente qué es Langchain y cómo se relaciona con Llama Index.\"\n",
    "print(f\"\\n--- Fase de Generación Aumentada para: '{pregunta_rag}' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta del sistema RAG:\n",
      "Langchain es un framework que permite la construcción de aplicaciones utilizando LLMs (Modelos de Lenguaje Profundos) mediante componentes modulares como cadenas, agentes y herramientas de memoria. También se puede integrar con LangGraph para crear flujos de trabajo RAG más complejos.\n",
      "\n",
      "En cuanto a Llama Index, no hay información directa sobre su relación con Langchain en el contexto proporcionado. Sin embargo, se menciona que Langchain se puede integrar con Llama Index para flujos de trabajo RAG más complejos, lo que sugiere que Langchain es compatible con Llama Index y ofrece una forma de expandir sus capacidades.\n",
      "\n",
      "En resumen, Langchain es un framework que ofrece componentes modulares para la construcción de aplicaciones con LLMs, mientras que su relación con Llama Index se centra en la integración de flujos de trabajo RAG más complejos.\n",
      "\n",
      "Nodos fuente utilizados para la respuesta:\n",
      "Score: 0.7453, Contenido: Langchain es otro framework popular para construir aplicaciones con LLMs.\n",
      "Ofrece componentes modular...\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    respuesta_rag = query_engine_rag.query(pregunta_rag)\n",
    "    print(f\"Respuesta del sistema RAG:\\n{respuesta_rag}\")\n",
    "\n",
    "    print(\"\\nNodos fuente utilizados para la respuesta:\")\n",
    "    for nodo_fuente in respuesta_rag.source_nodes:\n",
    "        print(f\"Score: {nodo_fuente.score:.4f}, Contenido: {nodo_fuente.text[:100]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la consulta RAG: {e}\")\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Consulta Directa al LLM (sin RAG) para: 'Explícame brevemente qué es Langchain y cómo se relaciona con Llama Index.' ---\n",
      "Respuesta directa del LLM:\n",
      "¡Hola! Me alegra explicarte sobre Langchain y su relación con Llama Index.\n",
      "\n",
      "**¿Qué es Langchain?**\n",
      "\n",
      "Langchain es una plataforma de software que permite a los desarrolladores crear modelos de lenguaje más sofisticados utilizando técnicas de procesamiento de lenguaje natural (NLP). Langchain se enfoca en proporcionar herramientas y frameworks para entrenar y utilizar modelos de lenguaje como si fueran \"paquetes\" reutilizables, lo que facilita la creación de aplicaciones de NLP más complejas.\n",
      "\n",
      "**¿Qué es Llama Index?**\n",
      "\n",
      "Llama Index es un subproyecto de Langchain que se enfoca en el desarrollo de índices de modelos de lenguaje (LLM). Los índices de modelo son estructuras de datos que permiten a los modelos de lenguaje acceder rápidamente a la información relevante y relacionada con una entrada de texto.\n",
      "\n",
      "**Relación entre Langchain y Llama Index**\n",
      "\n",
      "Langchain es el proyecto que rodea a Llama Index, ya que proporciona las herramientas y frameworks necesarios para entrenar y utilizar índices de modelo. En otras palabras, Llama Index es un subproyecto específico dentro del proyecto más amplio de Langchain.\n",
      "\n",
      "En resumen, Langchain es una plataforma de software que permite crear modelos de lenguaje más sofisticados, mientras que Llama Index es un subproyecto que se enfoca en el desarrollo de índices de modelo para los modelos de lenguaje.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "llm_directo = Settings.llm # Usamos el LLM configurado globalmente\n",
    "pregunta_directa_llm = \"Explícame brevemente qué es Langchain y cómo se relaciona con Llama Index.\"\n",
    "\n",
    "print(f\"\\n--- Consulta Directa al LLM (sin RAG) para: '{pregunta_directa_llm}' ---\")\n",
    "try:\n",
    "    respuesta_directa_llm = llm_directo.complete(pregunta_directa_llm)\n",
    "    print(f\"Respuesta directa del LLM:\\n{respuesta_directa_llm.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la consulta directa al LLM: {e}\")\n",
    "print(\"-------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
