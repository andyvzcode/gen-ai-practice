{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (0.3.28)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (0.3.12)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langgraph) (0.3.59)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langgraph) (2.0.24)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langgraph) (0.1.8)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langgraph) (0.1.61)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain) (0.3.30)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain-openai) (1.78.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Annotated, Sequence, Literal\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode # Para facilitar la creación de nodos de herramientas\n",
    "from langchain_core.tools import tool # Para crear herramientas fácilmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas definidas y vinculadas al LLM.\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def magic_search_tool(query: str) -> str:\n",
    "    \"\"\"Simula una búsqueda mágica. Si la consulta es sobre 'LangGraph', devuelve información específica.\n",
    "    Para otras consultas, indica que no encontró nada específico.\"\"\"\n",
    "    print(f\"---> Herramienta 'magic_search_tool' llamada con la consulta: {query}\")\n",
    "    if \"langgraph\" in query.lower():\n",
    "        return \"LangGraph es una biblioteca para construir aplicaciones LLM con estado y ciclos. Es útil para agentes complejos.\"\n",
    "    return f\"No se encontró información específica para '{query}' en la búsqueda mágica.\"\n",
    "\n",
    "tools = [magic_search_tool]\n",
    "\n",
    "# Langchain y LangGraph pueden trabajar con herramientas de forma más estructurada.\n",
    "# Aquí, vincularemos las herramientas al LLM para que sepa cuándo y cómo llamarlas.\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"Herramientas definidas y vinculadas al LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodos definidos.\n"
     ]
    }
   ],
   "source": [
    "def call_model(state: AgentState):\n",
    "    \"\"\"Llama al LLM con el historial de mensajes actual. \n",
    "    El LLM puede responder o solicitar una llamada a herramienta.\"\"\"\n",
    "    print(\"---NODO: call_model---\")\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    # Devolvemos la respuesta del LLM para añadirla al estado\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "print(\"Nodos definidos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lógica condicional definida.\n"
     ]
    }
   ],
   "source": [
    "def should_continue(state: AgentState) -> Literal[\"action\", \"end\"]:\n",
    "    \"\"\"Decide la siguiente acción basada en la última respuesta del LLM.\"\"\"\n",
    "    print(\"---NODO CONDICIONAL: should_continue---\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # Si el LLM hizo una llamada a herramienta, entonces vamos al nodo de acción (tool_node)\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        print(\"Decisión: LLM solicitó una herramienta. Ir a 'action'.\")\n",
    "        return \"action\"\n",
    "    # Si no hay llamada a herramienta, el LLM ha respondido, entonces terminamos.\n",
    "    print(\"Decisión: LLM respondió. Ir a 'end'.\")\n",
    "    return \"end\"\n",
    "\n",
    "print(\"Lógica condicional definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo compilado y listo.\n"
     ]
    }
   ],
   "source": [
    "# Crear el grafo de estado\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Añadir los nodos al grafo\n",
    "workflow.add_node(\"agent\", call_model) # Nodo donde el agente (LLM) decide\n",
    "workflow.add_node(\"action\", tool_node)  # Nodo que ejecuta la acción/herramienta\n",
    "\n",
    "# Definir el punto de entrada del grafo\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Añadir las aristas condicionales\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", # Nodo de origen\n",
    "    should_continue, # Función que decide la ruta\n",
    "    {\n",
    "        \"action\": \"action\", # Si should_continue devuelve \"action\", ir al nodo \"action\"\n",
    "        \"end\": END          # Si should_continue devuelve \"end\", terminar el grafo\n",
    "    }\n",
    ")\n",
    "\n",
    "# Añadir una arista desde el nodo de acción de vuelta al agente\n",
    "# Después de ejecutar una herramienta, queremos que el agente procese el resultado de la herramienta.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# Compilar el grafo en una aplicación ejecutable\n",
    "app = workflow.compile()\n",
    "print(\"Grafo compilado y listo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ejecución 1: Saludo Simple ---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM solicitó una herramienta. Ir a 'action'.\n",
      "Evento del grafo: Nodo='agent', Estado actualizado='{'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-05-16T17:34:57.240672Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6682104459, 'load_duration': 5542370209, 'prompt_eval_count': 192, 'prompt_eval_duration': 691344417, 'eval_count': 17, 'eval_duration': 447762125, 'model_name': 'llama3.2:latest'}, id='run--5a87c034-4604-473d-b509-bd05d9635da8-0', tool_calls=[{'name': 'magic_search_tool', 'args': {'query': ''}, 'id': 'd8d7766a-91bf-486c-becc-141714b0ee65', 'type': 'tool_call'}], usage_metadata={'input_tokens': 192, 'output_tokens': 17, 'total_tokens': 209})]}'\n",
      "---\n",
      "---> Herramienta 'magic_search_tool' llamada con la consulta: \n",
      "Evento del grafo: Nodo='action', Estado actualizado='{'messages': [ToolMessage(content=\"No se encontró información específica para '' en la búsqueda mágica.\", name='magic_search_tool', tool_call_id='d8d7766a-91bf-486c-becc-141714b0ee65')]}'\n",
      "---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM respondió. Ir a 'end'.\n",
      "Evento del grafo: Nodo='agent', Estado actualizado='{'messages': [AIMessage(content='Parece que no puedo completar tu solicitud en este momento debido a que no hay suficiente información sobre la que trabajar. ¿Podrías proporcionarme más contexto o detalles? Estoy aquí para ayudarte lo mejor que pueda.', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-05-16T17:34:58.782089Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1526125250, 'load_duration': 12175000, 'prompt_eval_count': 107, 'prompt_eval_duration': 165265833, 'eval_count': 49, 'eval_duration': 1347268542, 'model_name': 'llama3.2:latest'}, id='run--f10d8cb1-bb27-46af-a5e3-ba89e898def1-0', usage_metadata={'input_tokens': 107, 'output_tokens': 49, 'total_tokens': 156})]}'\n",
      "---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM solicitó una herramienta. Ir a 'action'.\n",
      "---> Herramienta 'magic_search_tool' llamada con la consulta: Ándole, Ül estás?\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM respondió. Ir a 'end'.\n",
      "Respuesta final (Saludo): Lo siento, no pude encontrar información relevante sobre \"Ul estás\" o \"Ándole\". ¿Puedes proporcionar más contexto o detalles sobre tu pregunta? Estoy aquí para ayudarte.\n",
      "\n",
      "--- Ejecución 2: Pregunta sobre LangGraph (debería usar herramienta) ---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM solicitó una herramienta. Ir a 'action'.\n",
      "Evento del grafo: Nodo='agent', Estado actualizado='{'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-05-16T17:35:02.623008Z', 'done': True, 'done_reason': 'stop', 'total_duration': 913118834, 'load_duration': 9911209, 'prompt_eval_count': 192, 'prompt_eval_duration': 379083625, 'eval_count': 19, 'eval_duration': 523736041, 'model_name': 'llama3.2:latest'}, id='run--8a5df8d3-e275-482a-a280-247ccc9fae37-0', tool_calls=[{'name': 'magic_search_tool', 'args': {'query': 'LangGraph'}, 'id': 'c51d64b1-419a-49e5-9aa5-f9ba9c017f6c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 192, 'output_tokens': 19, 'total_tokens': 211})]}'\n",
      "---\n",
      "---> Herramienta 'magic_search_tool' llamada con la consulta: LangGraph\n",
      "Evento del grafo: Nodo='action', Estado actualizado='{'messages': [ToolMessage(content='LangGraph es una biblioteca para construir aplicaciones LLM con estado y ciclos. Es útil para agentes complejos.', name='magic_search_tool', tool_call_id='c51d64b1-419a-49e5-9aa5-f9ba9c017f6c')]}'\n",
      "---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM respondió. Ir a 'end'.\n",
      "Evento del grafo: Nodo='agent', Estado actualizado='{'messages': [AIMessage(content='Parece que LangGraph es una herramienta de programación que permite crear aplicaciones basadas en modelos de lenguaje grande (LLM) con un enfoque en el manejo del estado y los ciclos. Algunas de sus características clave son:\\n\\n*   Permite a los desarrolladores crear agentes complejos con un enfoque en la gestión del estado y los ciclos.\\n*   Utiliza técnicas de procesamiento de lenguaje natural para generar respuestas coherentes y relevantes.\\n\\nLangGraph se puede utilizar para aplicaciones como chatbots, asistentes virtuales y sistemas de recomendación. Su capacidad para manejar el estado y los ciclos la convierte en una herramienta útil para desarrolladores que buscan crear experiencias de usuario más complejas y personalizadas.', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-05-16T17:35:07.762401Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5133964542, 'load_duration': 21490417, 'prompt_eval_count': 121, 'prompt_eval_duration': 235729250, 'eval_count': 173, 'eval_duration': 4872519750, 'model_name': 'llama3.2:latest'}, id='run--e1892b8e-7bd1-4971-bcdb-7765307c0e61-0', usage_metadata={'input_tokens': 121, 'output_tokens': 173, 'total_tokens': 294})]}'\n",
      "---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM solicitó una herramienta. Ir a 'action'.\n",
      "---> Herramienta 'magic_search_tool' llamada con la consulta: LangGraph\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM respondió. Ir a 'end'.\n",
      "Respuesta final (LangGraph): Parece que LangGraph es un entorno de desarrollo de software para crear modelos de lenguaje avanzados con características como el estado y los ciclos, lo cual es beneficioso para la creación de agentes más complejos.\n",
      "\n",
      "¿Quieres saber más sobre cómo se compara con otras bibliotecas similares?\n",
      "\n",
      "--- Ejecución 3: Pregunta general (no debería usar herramienta específica) ---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM solicitó una herramienta. Ir a 'action'.\n",
      "Evento del grafo: Nodo='agent', Estado actualizado='{'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-05-16T17:35:12.011346Z', 'done': True, 'done_reason': 'stop', 'total_duration': 956377083, 'load_duration': 9344000, 'prompt_eval_count': 194, 'prompt_eval_duration': 381547041, 'eval_count': 21, 'eval_duration': 565100375, 'model_name': 'llama3.2:latest'}, id='run--3528b945-b538-41f7-bcb7-49cd8aa1123e-0', tool_calls=[{'name': 'magic_search_tool', 'args': {'query': 'Capital de Francia'}, 'id': '4a2e01a7-ab1b-4a96-be6e-aaca7e064480', 'type': 'tool_call'}], usage_metadata={'input_tokens': 194, 'output_tokens': 21, 'total_tokens': 215})]}'\n",
      "---\n",
      "---> Herramienta 'magic_search_tool' llamada con la consulta: Capital de Francia\n",
      "Evento del grafo: Nodo='action', Estado actualizado='{'messages': [ToolMessage(content=\"No se encontró información específica para 'Capital de Francia' en la búsqueda mágica.\", name='magic_search_tool', tool_call_id='4a2e01a7-ab1b-4a96-be6e-aaca7e064480')]}'\n",
      "---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM respondió. Ir a 'end'.\n",
      "Evento del grafo: Nodo='agent', Estado actualizado='{'messages': [AIMessage(content='Lo siento, no pude encontrar la respuesta exacta a su pregunta. Sin embargo, puedo decirte que la capital de Francia es París. ¿Puedo ayudarte con algo más?', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-05-16T17:35:13.42671Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1412226792, 'load_duration': 9929792, 'prompt_eval_count': 118, 'prompt_eval_duration': 234130084, 'eval_count': 43, 'eval_duration': 1166213666, 'model_name': 'llama3.2:latest'}, id='run--ef1e40dc-81e8-45e1-8712-1ece07c334c4-0', usage_metadata={'input_tokens': 118, 'output_tokens': 43, 'total_tokens': 161})]}'\n",
      "---\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM solicitó una herramienta. Ir a 'action'.\n",
      "---> Herramienta 'magic_search_tool' llamada con la consulta: capital de Francia\n",
      "---NODO: call_model---\n",
      "---NODO CONDICIONAL: should_continue---\n",
      "Decisión: LLM respondió. Ir a 'end'.\n",
      "Respuesta final (Capital): La respuesta al tu pregunta es: París.\n"
     ]
    }
   ],
   "source": [
    "inputs1 = {\"messages\": [HumanMessage(content=\"Hola, ¿cómo estás?\")]}\n",
    "try:\n",
    "    print(\"\\n--- Ejecución 1: Saludo Simple ---\")\n",
    "    for event in app.stream(inputs1):\n",
    "        for key, value in event.items():\n",
    "            print(f\"Evento del grafo: Nodo='{key}', Estado actualizado='{value}'\")\n",
    "        print(\"---\")\n",
    "    # La respuesta final estará en el último estado del nodo 'agent' o 'end'\n",
    "    final_state_1 = app.invoke(inputs1)\n",
    "    print(f\"Respuesta final (Saludo): {final_state_1['messages'][-1].content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error en la ejecución 1: {e}\")\n",
    "\n",
    "\n",
    "# Ejecutar con una pregunta que podría requerir la herramienta\n",
    "inputs2 = {\"messages\": [HumanMessage(content=\"¿Qué sabes sobre LangGraph?\")]}\n",
    "try:\n",
    "    print(\"\\n--- Ejecución 2: Pregunta sobre LangGraph (debería usar herramienta) ---\")\n",
    "    for event in app.stream(inputs2, {\"recursion_limit\": 5}): # Añadir límite de recursión\n",
    "        for key, value in event.items():\n",
    "            print(f\"Evento del grafo: Nodo='{key}', Estado actualizado='{value}'\")\n",
    "        print(\"---\")\n",
    "    final_state_2 = app.invoke(inputs2, {\"recursion_limit\": 5})\n",
    "    print(f\"Respuesta final (LangGraph): {final_state_2['messages'][-1].content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error en la ejecución 2: {e}\")\n",
    "\n",
    "\n",
    "# Ejecutar con una pregunta que la herramienta no cubre\n",
    "inputs3 = {\"messages\": [HumanMessage(content=\"¿Cuál es la capital de Francia?\")]}\n",
    "try:\n",
    "    print(\"\\n--- Ejecución 3: Pregunta general (no debería usar herramienta específica) ---\")\n",
    "    for event in app.stream(inputs3, {\"recursion_limit\": 5}):\n",
    "        for key, value in event.items():\n",
    "            print(f\"Evento del grafo: Nodo='{key}', Estado actualizado='{value}'\")\n",
    "        print(\"---\")\n",
    "    final_state_3 = app.invoke(inputs3, {\"recursion_limit\": 5})\n",
    "    print(f\"Respuesta final (Capital): {final_state_3['messages'][-1].content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error en la ejecución 3: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
